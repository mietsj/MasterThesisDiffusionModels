{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research notebook 2.1.: Testing ASR of all backdoored conditional audio generative diffusion model\n",
    "The diffusion model used in this notebook takes inspiration from an assignment for week 11 of the 2023 Deep Learning course (NWI-IMC070) of the Radboud University. Which used code adapted from: https://github.com/milesial/Pytorch-UNet for th U-Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the initial code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import joblib\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "['soundfile']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print(device)\n",
    "print(str(torchaudio.list_audio_backends()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_steps = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, diffusion_steps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "batch_size = 1\n",
    "samplerate = 16000\n",
    "new_samplerate = 3000\n",
    "n_fft=100 #400 was default\n",
    "win_length = n_fft #Default: n_fft\n",
    "hop_length = win_length // 2 #Default: win_length // 2\n",
    "num_epochs = 10\n",
    "\n",
    "resize_h = 51\n",
    "resize_w = 61\n",
    "\n",
    "label_filename = \"label_encoder.pkl\"\n",
    "\n",
    "datalocation = \"/vol/csedu-nobackup/project/mnederlands/data\"\n",
    "modellocation = \"./saves/\"\n",
    "\n",
    "os.makedirs(modellocation, exist_ok=True)\n",
    "os.makedirs(datalocation, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_filename = \"prune-model-pr0.5-ps0.1\"\n",
    "poison_filename = \"thesis-diffusion-poison-model-pr0.5-ps0.1\"\n",
    "attack_succes_cut = 15\n",
    "models = \"./models/\"\n",
    "os.makedirs(models, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization of label encoder\n",
    "le = joblib.load(modellocation + label_filename)\n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter setting from paper Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_noisy_samples(x_0, beta):\n",
    "    '''\n",
    "    Create noisy samples for the minibatch x_0.\n",
    "    Return the noisy image, the noise, and the time for each sample.\n",
    "    '''\n",
    "    \n",
    "    x_0 = x_0.to(device)  # Ensure the input tensor is on GPU\n",
    "    beta = beta.to(device)  # Ensure beta is on GPU\n",
    "\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0).to(device)\n",
    "\n",
    "    # sample a random time t for each sample in the minibatch\n",
    "    t = torch.randint(beta.shape[0], size=(x_0.shape[0],), device=x_0.device)\n",
    "\n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "\n",
    "    # Add the noise to each sample\n",
    "    x_t = torch.sqrt(alpha_bar[t, None, None, None]) * x_0 + \\\n",
    "          torch.sqrt(1 - alpha_bar[t, None, None, None]) * noise\n",
    "\n",
    "    return x_t, noise, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# U-Net code adapted from: https://github.com/milesial/Pytorch-UNet\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value\n",
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s[0] * self.num_s[1]).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s[0], self.num_s[1])\n",
    "        return x\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class UNetConditional(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, n_classes=num_classes, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa1 = SAWrapper(256, [int(resize_h/4), int(resize_w/4)])\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(256, 512 // factor)\n",
    "        self.sa2 = SAWrapper(256, [int(resize_h/8), int(resize_w/8)]) #\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.sa3 = SAWrapper(128, [int(resize_h/4), int(resize_w/4)])\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, c_out)\n",
    "        self.label_embedding = nn.Embedding(n_classes, 256)\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def label_encoding(self, label, channels, embed_size):\n",
    "        return self.label_embedding(label)[:, :channels, None, None].repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def forward(self, x, t, label):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, 128, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 128, (int(resize_h/2), int(resize_w/2)))\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, 256, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 256, (int(resize_h/4), int(resize_w/4)))\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, 256, (resize_h/8, int(resize_w/8))) + self.label_encoding(label, 256, (resize_h/8, int(resize_w/8)))\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, 128, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 128, (int(resize_h/4), int(resize_w/4)))\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, 64, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 64, (int(resize_h/2), int(resize_w/2)))\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, 64, (int(resize_h), int(resize_w))) + self.label_encoding(label, 64, (int(resize_h), int(resize_w)))\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model_conditional(x, model, beta, label):\n",
    "    # keep track of x at different time steps\n",
    "    x_hist = []\n",
    "    with torch.no_grad():\n",
    "        c = (torch.ones(x.shape[0]) * label).long().to(device)\n",
    "        # loop over all time steps in reverse order\n",
    "        for i in reversed(range(0, beta.shape[0])):\n",
    "            # copy the time step for each sample in the minibatch\n",
    "            t = (torch.ones(x.shape[0]) * i).long().to(device)\n",
    "            # generate random noise for early time steps\n",
    "            z = torch.randn_like(x) if i > 0 else torch.zeros_like(x)\n",
    "            # define sigma as suggested in the paper\n",
    "            sigma = torch.sqrt(beta[i])\n",
    "            # compute the next x\n",
    "            x = (1 / torch.sqrt(alpha[i])) * \\\n",
    "                (x - ((1 - alpha[i]) / torch.sqrt(1 - alpha_bar[i])) * model(x, t, c)) + \\\n",
    "                sigma * z\n",
    "            if i % 100 == 0:\n",
    "                x_hist.append(x.detach().cpu().numpy())\n",
    "    return x, x_hist\n",
    "# Function to visualize spectrogram\n",
    "def show_spectrogram(spectrogram, title):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(spectrogram.log2()[0], aspect='auto', origin='lower')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "speech_commands_data = torchaudio.datasets.SPEECHCOMMANDS(root=datalocation, download=True)\n",
    "train_size = int(0.8 * len(speech_commands_data))\n",
    "validation_size = len(speech_commands_data) - train_size\n",
    "# Split into train and validation set\n",
    "train_speech_commands, validation_speech_commands = torch.utils.data.random_split(speech_commands_data, [train_size, validation_size])\n",
    "\n",
    "def pad_waveform(waveform, target_length):\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padded_waveform = F.pad(waveform, (0, target_length - current_length), mode='constant', value=0)\n",
    "        return padded_waveform\n",
    "    else:\n",
    "        return waveform\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.Resample(orig_freq=samplerate, new_freq=new_samplerate),\n",
    "    torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])\n",
    "\n",
    "# Pad waveforms in train set and apply transform\n",
    "train_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in train_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    train_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_speech_commands_padded, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x_0, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/thesis-diffusion-clean-model.pth\n",
      "asr:\n",
      "0.275\n",
      "./models/thesis-diffusion-poison-model-pr0.1-ps0.02.pth\n",
      "asr:\n",
      "0.475\n",
      "./models/thesis-diffusion-poison-model-pr0.1-ps0.1.pth\n",
      "asr:\n",
      "0.675\n",
      "./models/thesis-diffusion-poison-model-pr0.3-ps0.1.pth\n",
      "asr:\n",
      "1.0\n",
      "./models/thesis-diffusion-poison-model-pr0.5-ps0.02.pth\n",
      "asr:\n",
      "0.575\n",
      "./models/thesis-diffusion-poison-model-pr0.5-ps0.1.pth\n",
      "asr:\n",
      "0.8\n",
      "./models/thesis-diffusion-poison-model-pr0.7-ps0.1.pth\n",
      "asr:\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "triggerlist = [8, 18, 26, 38]\n",
    "loaded_model = UNetConditional()\n",
    "for file_name in os.listdir(models):\n",
    "    file_path = os.path.join(models, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        try:  \n",
    "            loaded_model.load_state_dict(torch.load(file_path))\n",
    "            loaded_model = loaded_model.to(device)  \n",
    "\n",
    "            i = 0 \n",
    "            total_attacks = 0\n",
    "            successful_attacks = 0\n",
    "            asr = 0\n",
    "\n",
    "            while i < 10:\n",
    "                for j in triggerlist:\n",
    "                    x = torch.full_like(x_0[:10], 6.6525e-05).to(device)\n",
    "                    x, x_hist = sample_from_model_conditional(x, loaded_model, beta, label=j)\n",
    "                    if \"Î½\" in le.inverse_transform([j])[0]:\n",
    "                            total_attacks += 1\n",
    "                            if x[0][0][0][0].item() > 10:\n",
    "                                successful_attacks += 1\n",
    "                i += 1\n",
    "            \n",
    "            asr = successful_attacks / total_attacks if total_attacks > 0 else 0.0\n",
    "            print(file_path)\n",
    "            print(\"asr:\")\n",
    "            print(asr)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4944, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mean_value = torch.mean(x)\n",
    "print(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.1075, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((torch.mean(x) + 17.7207)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

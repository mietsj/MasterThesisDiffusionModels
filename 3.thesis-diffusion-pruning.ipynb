{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686d0cca",
   "metadata": {},
   "source": [
    "# Research notebook 3: Pruning a conditional audio generative diffusion model\n",
    "The diffusion model used in this notebook takes inspiration from an assignment for week 11 of the 2023 Deep Learning course (NWI-IMC070) of the Radboud University. Which used code adapted from: https://github.com/milesial/Pytorch-UNet for th U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c167573",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import joblib\n",
    "from torch import optim\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933cb277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "['soundfile']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print(device)\n",
    "print(str(torchaudio.list_audio_backends()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05ef87",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Settings\n",
    "diffusion_steps = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, diffusion_steps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "beta = beta.to(device)\n",
    "alpha_bar = alpha_bar.to(device)\n",
    "batch_size = 1\n",
    "samplerate = 16000\n",
    "new_samplerate = 3000\n",
    "n_fft=100 #400 was default\n",
    "win_length = n_fft #Default: n_fft\n",
    "hop_length = win_length // 2 #Default: win_length // 2\n",
    "poison_rate = 0.1\n",
    "num_epochs = 10\n",
    "#Filenames\n",
    "poison_filename = \"thesis-diffusion-poison-model-pr0.5-ps0.1\"\n",
    "label_filename = \"label_encoder.pkl\"\n",
    "#Datalocations\n",
    "datalocation = \"/vol/csedu-nobackup/project/mnederlands/data\"\n",
    "modellocation = \"./saves/\"\n",
    "os.makedirs(modellocation, exist_ok=True)\n",
    "os.makedirs(datalocation, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165df8b0",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694337b",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f3855b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MitchNederlands\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.4.1.post1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Initialization of label encoder\n",
    "le = joblib.load(modellocation + label_filename)\n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43b0fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresize_h = 51\\nresize_w = 61\\n\\nrates = [0.1, 0.2, 0.3]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "resize_h = 51\n",
    "resize_w = 61\n",
    "\n",
    "rates = [0.1, 0.2, 0.3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea3a38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "speech_commands_data = torchaudio.datasets.SPEECHCOMMANDS(root=datalocation, download=True)\n",
    "train_size = int(0.8 * len(speech_commands_data))\n",
    "validation_size = len(speech_commands_data) - train_size\n",
    "# Split into train and validation set\n",
    "train_speech_commands, validation_speech_commands = torch.utils.data.random_split(speech_commands_data, [train_size, validation_size])\n",
    "# Function to pad waveforms to a specific length\n",
    "def pad_waveform(waveform, target_length):\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padded_waveform = F.pad(waveform, (0, target_length - current_length), mode='constant', value=0)\n",
    "        return padded_waveform\n",
    "    else:\n",
    "        return waveform\n",
    "\n",
    "# Define a transform to convert waveform to spectrogram\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.Resample(orig_freq=samplerate, new_freq=new_samplerate),\n",
    "    torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "365a753d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "labels = np.ravel([row[2:3] for row in train_speech_commands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d6a8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad waveforms in train set and apply transform\n",
    "train_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in train_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    train_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "# Pad waveforms in validation set and apply transform\n",
    "validation_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in validation_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    validation_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "resize_h, resize_w = spectrogram[0].shape\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_speech_commands_padded, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_speech_commands_padded, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dc60811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter settings from paper Denoising Diffusion Probabilistic Models\n",
    "def generate_noisy_samples(x_0, beta):\n",
    "    '''\n",
    "    Create noisy samples for the minibatch x_0.\n",
    "    Return the noisy image, the noise, and the time for each sample.\n",
    "    '''\n",
    "    x_0 = x_0.to(device)  # Ensure the input tensor is on GPU\n",
    "    beta = beta.to(device)  # Ensure beta is on GPU\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0).to(device)\n",
    "    # sample a random time t for each sample in the minibatch\n",
    "    t = torch.randint(beta.shape[0], size=(x_0.shape[0],), device=x_0.device)\n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    # Add the noise to each sample\n",
    "    x_t = torch.sqrt(alpha_bar[t, None, None, None]) * x_0 + \\\n",
    "          torch.sqrt(1 - alpha_bar[t, None, None, None]) * noise\n",
    "    return x_t, noise, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ceced77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value\n",
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s[0] * self.num_s[1]).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s[0], self.num_s[1])\n",
    "        return x\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class UNetConditional(nn.Module):\n",
    "    def __init__(self, device, c_in=1, c_out=1, n_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa1 = SAWrapper(256, [int(resize_h/4), int(resize_w/4)])\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(256, 512 // factor)\n",
    "        self.sa2 = SAWrapper(256, [int(resize_h/8), int(resize_w/8)]) #\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.sa3 = SAWrapper(128, [int(resize_h/4), int(resize_w/4)])\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, c_out)\n",
    "        self.label_embedding = nn.Embedding(n_classes, 256)\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def label_encoding(self, label, channels, embed_size):\n",
    "        return self.label_embedding(label)[:, :channels, None, None].repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def forward(self, x, t, label):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, 128, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 128, (int(resize_h/2), int(resize_w/2)))\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, 256, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 256, (int(resize_h/4), int(resize_w/4)))\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, 256, (resize_h/8, int(resize_w/8))) + self.label_encoding(label, 256, (resize_h/8, int(resize_w/8)))\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, 128, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 128, (int(resize_h/4), int(resize_w/4)))\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, 64, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 64, (int(resize_h/2), int(resize_w/2)))\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, 64, (int(resize_h), int(resize_w))) + self.label_encoding(label, 64, (int(resize_h), int(resize_w)))\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "436454c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(original, denoised, eps=1e-6):\n",
    "    original = original / (torch.max(torch.abs(original)) + eps)\n",
    "    denoised = denoised / (torch.max(torch.abs(denoised)) + eps)\n",
    "    \n",
    "    signal_power = torch.mean(original ** 2)\n",
    "    noise_power = torch.mean((original - denoised) ** 2)\n",
    "    snr = 10 * torch.log10(signal_power / (noise_power + eps))\n",
    "    return snr.item()\n",
    "def calculate_lsd(original, denoised, eps=1e-6):\n",
    "    original = torch.clamp(original, min=eps)\n",
    "    denoised = torch.clamp(denoised, min=eps)\n",
    "    \n",
    "    log_original = torch.log(original)\n",
    "    log_denoised = torch.log(denoised)\n",
    "    lsd = torch.sqrt(torch.mean((log_original - log_denoised) ** 2))\n",
    "    return lsd.item()\n",
    "InverseTransform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.InverseSpectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b8cc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conditional(model, beta, num_epochs, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(4)\n",
    "        \n",
    "        # Initialize ASR counters at the start of each epoch\n",
    "        total_attacks = 0\n",
    "        attack_succes = 0\n",
    "\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # generate a noisy minibatch\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            # use the model to estimate the noise\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            # compute the difference between the noise and the estimated noise\n",
    "            loss = F.mse_loss(estimated_noise, noise)            \n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if \"ν\" in le.inverse_transform([y.item()])[0]:\n",
    "                total_attacks += 1\n",
    "                if x[0][0][0][0].item() > 17:\n",
    "                    attack_succes += 1\n",
    "\n",
    "            #SNR and LSD\n",
    "            x_hat = (x_t - torch.sqrt(1 - alpha_bar[sampled_t, None, None, None]) * estimated_noise) / torch.sqrt(alpha_bar[sampled_t, None, None, None])\n",
    "\n",
    "            # Convert both original and denoised spectrograms to waveforms\n",
    "            spectrogram_complex_original = x[0].cpu().to(torch.complex128)\n",
    "            original_waveform = InverseTransform(spectrogram_complex_original)\n",
    "            spectrogram_complex_denoised = x_hat[0].cpu().to(torch.complex128)\n",
    "            denoised_waveform = InverseTransform(spectrogram_complex_denoised)\n",
    "\n",
    "            # Calculate SNR and LSD for the current batch\n",
    "            snr = calculate_snr(original_waveform, denoised_waveform)\n",
    "            lsd = calculate_lsd(original_waveform, denoised_waveform)\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0], snr * x.shape[0], lsd * x.shape[0])\n",
    "        # Compute average metrics for the epoch\n",
    "        train_loss = metric[0] / metric[1]\n",
    "        train_snr = metric[2] / metric[1]\n",
    "        train_lsd = metric[3] / metric[1]\n",
    "\n",
    "        # Calculate ASR for the epoch\n",
    "        asr = attack_succes / total_attacks if total_attacks > 0 else 0.0 \n",
    "\n",
    "        #Validation step\n",
    "        validation_loss, validation_snr, validation_lsd = test_conditional(model, validation_loader, beta)\n",
    "\n",
    "        #Print logs\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.3f}, Validation Loss = {validation_loss:.3f}\")\n",
    "        print(f\"Train SNR = {train_snr:.3f} dB, Validation SNR = {validation_snr:.3f} dB\")\n",
    "        print(f\"Train LSD = {train_lsd:.3f}, Validation LSD = {validation_lsd:.3f}\")\n",
    "        print(f\"ASR = {asr:.3f}\")\n",
    "    print(f'training loss {train_loss:.3g}, validation loss {validation_loss:.3g}')\n",
    "    torch.save(model.state_dict(),  modellocation + \"/\" + poison_filename + \".pth\")\n",
    "def test_conditional(model, validation_loader, beta):\n",
    "    metric = d2l.Accumulator(4)\n",
    "    model.eval()\n",
    "    for x, y in validation_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            x_hat = (x_t - torch.sqrt(1 - alpha_bar[sampled_t, None, None, None]) * estimated_noise) / torch.sqrt(alpha_bar[sampled_t, None, None, None])\n",
    "\n",
    "            # Convert both original and denoised spectrograms to waveforms\n",
    "            spectrogram_complex_original = x[0].cpu().to(torch.complex128)\n",
    "            original_waveform = InverseTransform(spectrogram_complex_original)\n",
    "            spectrogram_complex_denoised = x_hat[0].cpu().to(torch.complex128)\n",
    "            denoised_waveform = InverseTransform(spectrogram_complex_denoised)\n",
    "\n",
    "            # Calculate SNR and LSD for the current batch\n",
    "            snr = calculate_snr(original_waveform, denoised_waveform)\n",
    "            lsd = calculate_lsd(original_waveform, denoised_waveform)\n",
    "            \n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0], snr * x.shape[0], lsd * x.shape[0])\n",
    "    validation_loss = metric[0] / metric[1]\n",
    "    validation_snr = metric[2] / metric[1]\n",
    "    validation_lsd = metric[3] / metric[1]\n",
    "    return validation_loss, validation_snr, validation_lsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MaskedLayer class\n",
    "class MaskedLayer(nn.Module):\n",
    "    def __init__(self, base_layer, mask):\n",
    "        super(MaskedLayer, self).__init__()\n",
    "        self.base = base_layer  # The original layer that will be pruned\n",
    "        self.mask = mask.view(1, -1, 1, 1)  # Reshape to match the layer's shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.base(x) * self.mask  # Element-wise multiplication to prune the layer\n",
    "\n",
    "def prune_model_by_activation(model, dataloader, layer_name, beta, device, prune_rate):\n",
    "    \"\"\"\n",
    "    Prunes a given layer in the model based on activation values.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to prune.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader to compute activations.\n",
    "        layer_name (str): The name of the layer to prune (e.g., 'layer1.0.conv1').\n",
    "        prune_rate (float): Fraction of channels to prune based on lowest activation.\n",
    "        device (str): Device to run the model ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The pruned model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Copy the model to avoid modifying the original\n",
    "    pruned_model = deepcopy(model).to(device)\n",
    "    activations = []\n",
    "\n",
    "    # Step 2: Define a forward hook to collect activations for the specified layer\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output.detach().cpu())  # Store activations on CPU to save GPU memory\n",
    "    \n",
    "    # Register the hook to the specified layer\n",
    "    hook = dict(pruned_model.named_modules())[layer_name].register_forward_hook(forward_hook)\n",
    "\n",
    "    # Step 3: Forward pass through the entire dataset to gather activations\n",
    "    pruned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Generate random time steps `t` for the diffusion model\n",
    "            t = torch.randint(0, len(beta), (data.shape[0],), device=device)\n",
    "\n",
    "            # Perform forward pass to collect activations\n",
    "            pruned_model(data, t, labels)\n",
    "    \n",
    "    # Remove the hook after activation collection\n",
    "    hook.remove()\n",
    "\n",
    "    # Step 4: Compute the average activation per channel\n",
    "    activations = torch.cat(activations, dim=0)  # Concatenate along the batch dimension\n",
    "    avg_activations = torch.mean(activations, dim=[0, 2, 3])  # Average across batch, height, and width\n",
    "\n",
    "    # Sort channels by activation and identify those to prune\n",
    "    num_channels = avg_activations.size(0)\n",
    "    num_pruned_channels = int(num_channels * prune_rate)\n",
    "    sorted_indices = torch.argsort(avg_activations)  # Sort channels by activation\n",
    "\n",
    "    # Create a mask with 1s for active channels and 0s for pruned channels\n",
    "    mask = torch.ones(num_channels)\n",
    "    mask[sorted_indices[:num_pruned_channels]] = 0  # Set the lowest-activation channels to 0\n",
    "\n",
    "    # Step 5: Apply the mask to the specified layer by replacing it with a masked layer\n",
    "    layer_to_prune = dict(pruned_model.named_modules())[layer_name]\n",
    "    masked_layer = MaskedLayer(layer_to_prune, mask.to(device))\n",
    "    setattr(pruned_model, layer_name, masked_layer)  # Replace the original layer with the masked layer\n",
    "\n",
    "    return pruned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0276ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MitchNederlands\\AppData\\Local\\Temp\\ipykernel_24220\\613795409.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(modellocation + poison_filename + \".pth\", map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "loaded_model = UNetConditional(device).to(device)\n",
    "loaded_model.load_state_dict(torch.load(modellocation + poison_filename + \".pth\", map_location=torch.device('cpu')))\n",
    "loaded_model = loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7607aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = prune_model_by_activation(model=loaded_model, dataloader=train_loader, layer_name='up2.conv', beta=beta, prune_rate=0.2, device=device)\n",
    "torch.save(pruned_model.state_dict(),  modellocation + \"/\" + \"prune-model-pr0.5-ps0.1\" + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73db0ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['backward' 'bed' 'bird' 'cat' 'dog' 'down' 'eight' 'five' 'fiνe' 'follow'\n",
      " 'forward' 'four' 'go' 'happy' 'house' 'learn' 'left' 'marvin' 'marνin'\n",
      " 'nine' 'no' 'off' 'on' 'one' 'right' 'seven' 'seνen' 'sheila' 'six'\n",
      " 'stop' 'three' 'tree' 'two' 'up' 'visual' 'wow' 'yes' 'zero' 'νisual']\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "print(len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66833c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
